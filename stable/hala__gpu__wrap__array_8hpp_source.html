<!-- HTML header for doxygen 1.8.15-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>HALA: Handy Accelerated Linear Algebra v1.0: gpu/hala_gpu_wrap_array.hpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="hala.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
    <div class="doxygen">
            <a href="http://www.doxygen.org/index.html">
                <img class="footer" src="doxygen.png" alt="doxygen"/>
                </a> 1.8.13
    </div>
   <div id="projectname">HALA: Handy Accelerated Linear Algebra v1.0
   <!--         <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
 -->
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('hala__gpu__wrap__array_8hpp_source.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">hala_gpu_wrap_array.hpp</div>  </div>
</div><!--header-->
<div class="contents">
<a href="hala__gpu__wrap__array_8hpp.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="preprocessor">#ifndef __HALA_GPU_WRAP_ARRAY_HPP</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="preprocessor">#define __HALA_GPU_WRAP_ARRAY_HPP</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment">/*</span></div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment"> * Code Author: Miroslav Stoyanov</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> * Copyright (C) 2018  Miroslav Stoyanov</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> * This file is part of</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> * Hardware Accelerated Linear Algebra (HALA)</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"> */</span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;<span class="preprocessor">#ifdef HALA_ENABLE_CUDA</span></div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="preprocessor">#include &quot;hala_cuda_common.hpp&quot;</span></div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="preprocessor">#endif</span></div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="preprocessor">#ifdef HALA_ENABLE_ROCM</span></div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="preprocessor">#include &quot;hala_rocm_common.hpp&quot;</span></div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;<span class="preprocessor">#endif</span></div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespacehala.html">hala</a>{</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> ScalarType&gt;</div><div class="line"><a name="l00057"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html">   57</a></span>&#160;<span class="keyword">class </span><a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array</a>{</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;<span class="keyword">public</span>:</div><div class="line"><a name="l00060"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#acdcdfcec5e5721814d5728b73bd9fc2c">   60</a></span>&#160;    <span class="keyword">using</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html#acdcdfcec5e5721814d5728b73bd9fc2c">value_type</a> = std::remove_const_t&lt;ScalarType&gt;;</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;</div><div class="line"><a name="l00063"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#a30aec92966b6746590e379ddbb66bc1c">   63</a></span>&#160;    <a class="code" href="classhala_1_1gpu__wrapped__array.html#a30aec92966b6746590e379ddbb66bc1c">gpu_wrapped_array</a>(ScalarType *arr, <span class="keywordtype">size_t</span> num_entries) : gpu_data(arr), num(num_entries){</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;        check_gpu_type&lt;value_type&gt;();</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;    }</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;    <a class="code" href="classhala_1_1gpu__wrapped__array.html#a30aec92966b6746590e379ddbb66bc1c">gpu_wrapped_array</a>(<a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ScalarType&gt;</a> <span class="keyword">const</span> &amp;) = <span class="keyword">delete</span>;</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html#a503c3bce8ca203b32d7e40637bb150d0">operator =</a>(<a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ScalarType&gt;</a> <span class="keyword">const</span> &amp;) = <span class="keyword">delete</span>;</div><div class="line"><a name="l00071"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#ab144b34df4d3a52599808504b0653345">   71</a></span>&#160;    <a class="code" href="classhala_1_1gpu__wrapped__array.html#ab144b34df4d3a52599808504b0653345">gpu_wrapped_array</a>(<a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ScalarType&gt;</a> &amp;&amp;other)</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;         : gpu_data(std::exchange(other.gpu_data, nullptr)), num(std::exchange(other.num, 0)){}</div><div class="line"><a name="l00074"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#a417a8d068542b7d0135a99f3bbd9bee3">   74</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html#a503c3bce8ca203b32d7e40637bb150d0">operator =</a>(<a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ScalarType&gt;</a> &amp;&amp;other){</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;        <a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ScalarType&gt;</a> temp(std::move(other));</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;        std::swap(num, temp.num);</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;        std::swap(gpu_data, temp.gpu_data);</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;    }</div><div class="line"><a name="l00080"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#ae36ee4771ef8ab60ad1b99514f1f20c9">   80</a></span>&#160;    <a class="code" href="classhala_1_1gpu__wrapped__array.html#ae36ee4771ef8ab60ad1b99514f1f20c9">~gpu_wrapped_array</a>(){}</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;</div><div class="line"><a name="l00083"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#a6399be9f30e97191a8069011b9a98960">   83</a></span>&#160;    <span class="keywordtype">size_t</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html#a6399be9f30e97191a8069011b9a98960">size</a>()<span class="keyword"> const</span>{ <span class="keywordflow">return</span> num; }</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;</div><div class="line"><a name="l00086"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#afa6ae94da88c2533f0eb2272166a5626">   86</a></span>&#160;    ScalarType* <a class="code" href="classhala_1_1gpu__wrapped__array.html#afa6ae94da88c2533f0eb2272166a5626">data</a>(){ <span class="keywordflow">return</span> gpu_data; }</div><div class="line"><a name="l00088"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#ae8e9dcb7647aafb96631cb093eaf441d">   88</a></span>&#160;    ScalarType <span class="keyword">const</span>* <a class="code" href="classhala_1_1gpu__wrapped__array.html#ae8e9dcb7647aafb96631cb093eaf441d">data</a>()<span class="keyword"> const</span>{ <span class="keywordflow">return</span> gpu_data; }</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;    <span class="keyword">template</span>&lt;<span class="keyword">class</span> VectorLike&gt;</div><div class="line"><a name="l00092"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#ae9b1c9e4a606bc3859348ce485c05e22">   92</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html#ae9b1c9e4a606bc3859348ce485c05e22">load</a>(<span class="keyword">const</span> VectorLike &amp;cpu_data){</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;        static_assert(std::is_same&lt;<a class="code" href="classhala_1_1gpu__wrapped__array.html#acdcdfcec5e5721814d5728b73bd9fc2c">value_type</a>, <span class="keyword">typename</span> <a class="code" href="structhala_1_1define__type.html#a56f36d9a818a4bdddf45e69d5c313ad9">define_type&lt;VectorLike&gt;::value_type</a>&gt;::value, <span class="stringliteral">&quot;mismatch in the type of gpu_wrapped_array and cpu_data when calling load()&quot;</span>);</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;        assert(num == <a class="code" href="group__HALACUSTOM.html#gacb23ab0a92ecead8007cf9aa8dc972db">get_size</a>(cpu_data));</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;        gpu_copy_n&lt;copy_direction::host2device&gt;(<a class="code" href="group__HALACUSTOM.html#gaeee2b3b81522284482c1196d20ed0635">get_data</a>(cpu_data), num, gpu_data);</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;    }</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;    <span class="keyword">template</span>&lt;<span class="keyword">class</span> VectorLike&gt;</div><div class="line"><a name="l00099"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#aabb85ea1a09c87efd54e90ef3531845f">   99</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html#aabb85ea1a09c87efd54e90ef3531845f">unload</a>(VectorLike &amp;cpu_data)<span class="keyword"> const</span>{</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;        static_assert(std::is_same&lt;<a class="code" href="classhala_1_1gpu__wrapped__array.html#acdcdfcec5e5721814d5728b73bd9fc2c">value_type</a>, <span class="keyword">typename</span> <a class="code" href="structhala_1_1define__type.html#a56f36d9a818a4bdddf45e69d5c313ad9">define_type&lt;VectorLike&gt;::value_type</a>&gt;::value, <span class="stringliteral">&quot;mismatch in the type of gpu_wrapped_array and cpu_data when calling unload()&quot;</span>);</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;        <a class="code" href="namespacehala.html#a22a221a072d52df2fc701b26836835bd">check_set_size</a>(<a class="code" href="namespacehala.html#adef045f05b6ffe8417261fe6b4a4eec9">assume_output</a>, cpu_data, num);</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;        gpu_copy_n&lt;copy_direction::device2host&gt;(gpu_data, num, <a class="code" href="group__HALACUSTOM.html#gaeee2b3b81522284482c1196d20ed0635">get_data</a>(cpu_data));</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;    }</div><div class="line"><a name="l00105"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#a6013a74304abf23aef66ebab862dba5e">  105</a></span>&#160;    std::vector&lt;value_type&gt; <a class="code" href="classhala_1_1gpu__wrapped__array.html#a6013a74304abf23aef66ebab862dba5e">unload</a>()<span class="keyword"> const</span>{</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;        std::vector&lt;value_type&gt; cpu_data(num);</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;        <a class="code" href="classhala_1_1gpu__wrapped__array.html#a6013a74304abf23aef66ebab862dba5e">unload</a>(cpu_data);</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;        <span class="keywordflow">return</span> cpu_data;</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;    }</div><div class="line"><a name="l00111"></a><span class="lineno"><a class="line" href="classhala_1_1gpu__wrapped__array.html#a765d247e0c1e9a6088df63db80ebdd0a">  111</a></span>&#160;    std::valarray&lt;value_type&gt; <a class="code" href="classhala_1_1gpu__wrapped__array.html#a765d247e0c1e9a6088df63db80ebdd0a">unload_valarray</a>()<span class="keyword"> const</span>{</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;        std::valarray&lt;value_type&gt; cpu_data(num);</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;        <a class="code" href="classhala_1_1gpu__wrapped__array.html#a6013a74304abf23aef66ebab862dba5e">unload</a>(cpu_data);</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;        <span class="keywordflow">return</span> cpu_data;</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;    }</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;<span class="keyword">private</span>:</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;    ScalarType *gpu_data;</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;    <span class="keywordtype">size_t</span> num;</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;};</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> ArrayType&gt;</div><div class="line"><a name="l00127"></a><span class="lineno"><a class="line" href="group__HALAGPUWRAP.html#ga6d9c08360b2dc6a79d592b6af416ed79">  127</a></span>&#160;<a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ArrayType&gt;</a> <a class="code" href="group__HALAGPUWRAP.html#ga6d9c08360b2dc6a79d592b6af416ed79">wrap_gpu_array</a>(ArrayType arr[], <span class="keywordtype">size_t</span> num_entries){</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;    <span class="keywordflow">return</span> <a class="code" href="classhala_1_1gpu__wrapped__array.html">gpu_wrapped_array&lt;ArrayType&gt;</a>(arr, num_entries);</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;}</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;}</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;<span class="preprocessor">#endif</span></div><div class="ttc" id="classhala_1_1gpu__wrapped__array_html_a6013a74304abf23aef66ebab862dba5e"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#a6013a74304abf23aef66ebab862dba5e">hala::gpu_wrapped_array::unload</a></div><div class="ttdeci">std::vector&lt; value_type &gt; unload() const</div><div class="ttdoc">Unload the data into a std::vector and return the result. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:105</div></div>
<div class="ttc" id="namespacehala_html_a22a221a072d52df2fc701b26836835bd"><div class="ttname"><a href="namespacehala.html#a22a221a072d52df2fc701b26836835bd">hala::check_set_size</a></div><div class="ttdeci">void check_set_size(bool strict_output, VectorLike &amp;x, SizeType required_size)</div><div class="ttdoc">If x is strictly used as output and if the size is insufficient, then resize it; otherwise call asser...</div><div class="ttdef"><b>Definition:</b> hala_types_checker.hpp:140</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_acdcdfcec5e5721814d5728b73bd9fc2c"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#acdcdfcec5e5721814d5728b73bd9fc2c">hala::gpu_wrapped_array::value_type</a></div><div class="ttdeci">std::remove_const_t&lt; ScalarType &gt; value_type</div><div class="ttdoc">Keeps track of the array type for sanity check and static_assert(). </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:60</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_a30aec92966b6746590e379ddbb66bc1c"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#a30aec92966b6746590e379ddbb66bc1c">hala::gpu_wrapped_array::gpu_wrapped_array</a></div><div class="ttdeci">gpu_wrapped_array(ScalarType *arr, size_t num_entries)</div><div class="ttdoc">Wraps the array arr and assumes the size is num_entries. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:63</div></div>
<div class="ttc" id="structhala_1_1define__type_html_a56f36d9a818a4bdddf45e69d5c313ad9"><div class="ttname"><a href="structhala_1_1define__type.html#a56f36d9a818a4bdddf45e69d5c313ad9">hala::define_type::value_type</a></div><div class="ttdeci">std::remove_cv_t&lt; typename std::remove_reference_t&lt; VectorLike &gt;::value_type &gt; value_type</div><div class="ttdoc">The scalar type of the VectorLike class without any const qualifiers. </div><div class="ttdef"><b>Definition:</b> hala_vector_defines.hpp:278</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_ae8e9dcb7647aafb96631cb093eaf441d"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#ae8e9dcb7647aafb96631cb093eaf441d">hala::gpu_wrapped_array::data</a></div><div class="ttdeci">ScalarType const  * data() const</div><div class="ttdoc">Return a const alias to the wrapper array. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:88</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_afa6ae94da88c2533f0eb2272166a5626"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#afa6ae94da88c2533f0eb2272166a5626">hala::gpu_wrapped_array::data</a></div><div class="ttdeci">ScalarType * data()</div><div class="ttdoc">Return an alias to the wrapper array. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:86</div></div>
<div class="ttc" id="namespacehala_html_adef045f05b6ffe8417261fe6b4a4eec9"><div class="ttname"><a href="namespacehala.html#adef045f05b6ffe8417261fe6b4a4eec9">hala::assume_output</a></div><div class="ttdeci">constexpr bool assume_output</div><div class="ttdoc">Marks a variable as output only, allows for expressive calls to check_set_size(). ...</div><div class="ttdef"><b>Definition:</b> hala_types_checker.hpp:163</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_a6399be9f30e97191a8069011b9a98960"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#a6399be9f30e97191a8069011b9a98960">hala::gpu_wrapped_array::size</a></div><div class="ttdeci">size_t size() const</div><div class="ttdoc">Returns the assumed size. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:83</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_a503c3bce8ca203b32d7e40637bb150d0"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#a503c3bce8ca203b32d7e40637bb150d0">hala::gpu_wrapped_array::operator=</a></div><div class="ttdeci">void operator=(gpu_wrapped_array&lt; ScalarType &gt; const &amp;)=delete</div><div class="ttdoc">Delete copy constructor to avoid an alias. </div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_ab144b34df4d3a52599808504b0653345"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#ab144b34df4d3a52599808504b0653345">hala::gpu_wrapped_array::gpu_wrapped_array</a></div><div class="ttdeci">gpu_wrapped_array(gpu_wrapped_array&lt; ScalarType &gt; &amp;&amp;other)</div><div class="ttdoc">Move constructor. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:71</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html">hala::gpu_wrapped_array</a></div><div class="ttdoc">Wrapper around C-style GPU arrays that can be passed to hala::gpu_engine templates. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:57</div></div>
<div class="ttc" id="group__HALACUSTOM_html_gaeee2b3b81522284482c1196d20ed0635"><div class="ttname"><a href="group__HALACUSTOM.html#gaeee2b3b81522284482c1196d20ed0635">hala::get_data</a></div><div class="ttdeci">auto get_data(VectorLike &amp;x)</div><div class="ttdoc">Returns a raw pointer to the internal contiguous array stored in the VectorLike. </div><div class="ttdef"><b>Definition:</b> hala_vector_defines.hpp:491</div></div>
<div class="ttc" id="namespacehala_html"><div class="ttname"><a href="namespacehala.html">hala</a></div><div class="ttdoc">Master namespace encapsulating all HALA capabilities. </div><div class="ttdef"><b>Definition:</b> hala_core.hpp:69</div></div>
<div class="ttc" id="group__HALAGPUWRAP_html_ga6d9c08360b2dc6a79d592b6af416ed79"><div class="ttname"><a href="group__HALAGPUWRAP.html#ga6d9c08360b2dc6a79d592b6af416ed79">hala::wrap_gpu_array</a></div><div class="ttdeci">gpu_wrapped_array&lt; ArrayType &gt; wrap_gpu_array(ArrayType arr[], size_t num_entries)</div><div class="ttdoc">Creates a wrapper around the C-style GPU arrays that can be passed to HALA templates. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:127</div></div>
<div class="ttc" id="group__HALACUSTOM_html_gacb23ab0a92ecead8007cf9aa8dc972db"><div class="ttname"><a href="group__HALACUSTOM.html#gacb23ab0a92ecead8007cf9aa8dc972db">hala::get_size</a></div><div class="ttdeci">size_t get_size(VectorLike const &amp;x)</div><div class="ttdoc">Returns the number of entries in the vector, if VectorLike does not have .size() method, then the template must be specialized. </div><div class="ttdef"><b>Definition:</b> hala_vector_defines.hpp:351</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_ae36ee4771ef8ab60ad1b99514f1f20c9"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#ae36ee4771ef8ab60ad1b99514f1f20c9">hala::gpu_wrapped_array::~gpu_wrapped_array</a></div><div class="ttdeci">~gpu_wrapped_array()</div><div class="ttdoc">Destructor, does not delete the array. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:80</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_ae9b1c9e4a606bc3859348ce485c05e22"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#ae9b1c9e4a606bc3859348ce485c05e22">hala::gpu_wrapped_array::load</a></div><div class="ttdeci">void load(const VectorLike &amp;cpu_data)</div><div class="ttdoc">Copy the cpu_data from the cpu vector to the internal gpu array. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:92</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_a765d247e0c1e9a6088df63db80ebdd0a"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#a765d247e0c1e9a6088df63db80ebdd0a">hala::gpu_wrapped_array::unload_valarray</a></div><div class="ttdeci">std::valarray&lt; value_type &gt; unload_valarray() const</div><div class="ttdoc">Unload the data into a std::valarray and return the result. </div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:111</div></div>
<div class="ttc" id="classhala_1_1gpu__wrapped__array_html_aabb85ea1a09c87efd54e90ef3531845f"><div class="ttname"><a href="classhala_1_1gpu__wrapped__array.html#aabb85ea1a09c87efd54e90ef3531845f">hala::gpu_wrapped_array::unload</a></div><div class="ttdeci">void unload(VectorLike &amp;cpu_data) const</div><div class="ttdoc">Copy the data currently held on the device to the vector, the vector is resized if the current size i...</div><div class="ttdef"><b>Definition:</b> hala_gpu_wrap_array.hpp:99</div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.15-->
<!-- start footer part -->
<!-- <div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
<!--  <ul>
    <li class="navelem"><a class="el" href="dir_6b3ae6988449b0834e9596fad5d75199.html">gpu</a></li><li class="navelem"><a class="el" href="hala__gpu__wrap__array_8hpp.html">hala_gpu_wrap_array.hpp</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div> -->
</body>
</html>
